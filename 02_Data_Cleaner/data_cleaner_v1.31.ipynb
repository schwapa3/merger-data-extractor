{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648ce12d-4878-4200-93fa-40422cadbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Script for cleaning the output data from web_scraper_vx.xx\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Author: Patrik Schwalm\n",
    "# E-Mail: schwapa3@students.zhaw.ch\n",
    "# Last update: 10.05.2022\n",
    "# Version 1.31\n",
    "\n",
    "#------------------------------\n",
    "# Setup\n",
    "#------------------------------\n",
    "# Install Python (Ananconda)\n",
    "# Install the necessary Python libraries (see Python libraries)\n",
    "# Create a JSON-file with the notebook web_scraper_vx.xx.ipynb\n",
    "# Save the JSON-file in the in the working directory with the name 'raw_data.json'\n",
    "\n",
    "#------------------------------\n",
    "# README\n",
    "#------------------------------\n",
    "# The cleaned JSON-files are saved in the working directoory with the name 'clean_train_data.json' and 'clean_input_data.json'\n",
    "# Only decision texts in english can be cleaned\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Python libraries\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Read data from JSON-file\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def get_json_file(filename = 'raw_data.json'):\n",
    "    # read JSON-file\n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return(data)\n",
    "        \n",
    "#-------------------------------------------------------------------------------------\n",
    "# Clean company names\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def clean_company_names(data):\n",
    "    \n",
    "    # Remove additions such as: 'SYNTHELABO (Art.14 proc.) (see M.1397 and M.1542)' from the company names\n",
    "    regex_company_names = [\n",
    "        ('\\([Aa]rt.[0-9a-zA-Z_\\s.]*\\)', ''),\n",
    "        ('\\([Ss]ee[0-9a-zA-Z_\\s.]*\\)', ''),\n",
    "        ('\\\\\"', ''),\n",
    "        ('^\\s*', ''),\n",
    "        ('\\s*$', '')\n",
    "    ]\n",
    "\n",
    "    for merger in data['mergers']:\n",
    "        for company_index, company in enumerate(merger['companies']):\n",
    "            for regex_company_name, new in regex_company_names:\n",
    "                company = re.sub(regex_company_name, new, company)\n",
    "            merger['companies'][company_index] = company\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Clean texts of some (cid:)\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def clean_text_of_unprintable_chars(data):\n",
    "    \n",
    "    # Only (cid:133) is needed for the sentenicer in the data_analyzer, the others are just cosmetics\n",
    "    regex_decision_texts = [\n",
    "        (\"\\(cid:146\\)\", \"'\"),\n",
    "        ('\\(cid:147\\)', '\"'),\n",
    "        ('\\(cid:148\\)', '\"'),\n",
    "        ('\\(cid:133\\)', '...'),\n",
    "        ('\\(cid:150\\)', '-')\n",
    "    ]\n",
    "    \n",
    "    for merger in data['mergers']:\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            for decision_text_index, decision_text in enumerate(decision['decision texts']):\n",
    "                text = decision_text['text']\n",
    "                for regex_decision_text, new in regex_decision_texts:\n",
    "                    text = re.sub(regex_decision_text, new, text)\n",
    "                decision_text['text'] = text\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# delete all decision texts except the english ones\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def get_en_decision_texts_only(data):\n",
    "    \n",
    "    merger_indexes = []\n",
    "    decision_indexes = []\n",
    "    decision_text_indexes = []\n",
    "    \n",
    "    for merger_index, merger in enumerate(data['mergers']):\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            for decision_text_index, decision_text in enumerate(decision['decision texts']):\n",
    "                if(\n",
    "                    decision_text['language'] != \"en\" \\\n",
    "                    or \"Error - PDF-file is not searchable\" in decision_text['text'] \\\n",
    "                    or \"Error - link does not point to a valid PDF-file\" in decision_text['text']\n",
    "                ):\n",
    "                    merger_indexes.append(merger_index)\n",
    "                    decision_indexes.append(decision_index)\n",
    "                    decision_text_indexes.append(decision_text_index)\n",
    "    \n",
    "    \n",
    "    merger_indexes.sort(reverse=True)\n",
    "    decision_indexes = decision_indexes[::-1]\n",
    "    decision_text_indexes = decision_text_indexes[::-1]\n",
    "    \n",
    "    for index in range(len(merger_indexes)):\n",
    "        data['mergers'][merger_indexes[index]]['decisions'][decision_indexes[index]]['decision texts'].pop(decision_text_indexes[index])\n",
    "                            \n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# remove all \\n (only tested with en decision texts)\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def clean_decision_texts_of_newline(data):\n",
    "    \n",
    "    for merger in data['mergers']:\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            for decision_text_index, decision_text in enumerate(decision['decision texts']):\n",
    "                # This function is based on: https://stackoverflow.com/a/37001613\n",
    "                decision_text['text'] = ' '.join(decision_text['text'].split())\n",
    "                \n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# remove all all merger cases without a decision text\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def remove_mergers_without_decision_texts(data):\n",
    "    \n",
    "    merger_indexes = []\n",
    "    decision_indexes = []\n",
    "    \n",
    "    for merger_index, merger in enumerate(data['mergers']):\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            if not decision['decision texts']:\n",
    "                merger_indexes.append(merger_index)\n",
    "                decision_indexes.append(decision_index)\n",
    "                \n",
    "    merger_indexes.sort(reverse=True)\n",
    "    decision_indexes = decision_indexes[::-1]\n",
    "    \n",
    "    for index in range(len(merger_indexes)):\n",
    "        data['mergers'][merger_indexes[index]]['decisions'].pop(decision_indexes[index])\n",
    "    \n",
    "    \n",
    "    del_merger_indexes = []\n",
    "    \n",
    "    for del_merger_index, merger in enumerate(data['mergers']):\n",
    "        if not merger['decisions']:\n",
    "            del_merger_indexes.append(del_merger_index)\n",
    "    \n",
    "    del_merger_indexes.sort(reverse=True)\n",
    "    \n",
    "    for index in range(len(del_merger_indexes)):\n",
    "        data['mergers'].pop(del_merger_indexes[index])\n",
    "            \n",
    "    return(data)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Write data to JSON-file\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def write_to_json_file(data, filename):\n",
    "    # Write all the scraped data in the JSON-file\n",
    "    with open(filename+'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent = 4)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create a json-fille for training data for the data labler\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def create_en_train_data():\n",
    "    data = get_json_file()\n",
    "    data = clean_company_names(data)\n",
    "    data = get_en_decision_texts_only(data)\n",
    "    data = clean_decision_texts_of_newline(data)\n",
    "    data = remove_mergers_without_decision_texts(data)\n",
    "    data = clean_text_of_unprintable_chars(data)\n",
    "    write_to_json_file(data, 'clean_train_data')\n",
    "    \n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create clean input data for the model\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def create_en_analysis_data():\n",
    "    data = get_json_file()\n",
    "    data = clean_company_names(data)\n",
    "    data = get_en_decision_texts_only(data)\n",
    "    data = clean_decision_texts_of_newline(data)\n",
    "    data = clean_text_of_unprintable_chars(data)\n",
    "    write_to_json_file(data, 'clean_input_data')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2153af0b-22a1-4d1d-a3a2-1a36cd50f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_en_train_data()\n",
    "create_en_analysis_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
