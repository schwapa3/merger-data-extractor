{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7125d81d-1dd2-451a-a299-0eaf7d03eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Script for extracting items from decision texts\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Author: Patrik Schwalm\n",
    "# E-Mail: schwapa3@students.zhaw.ch\n",
    "# Last update: 15.05.2022\n",
    "# Version 1.21\n",
    "\n",
    "#------------------------------\n",
    "# Setup\n",
    "#------------------------------\n",
    "# Install Python (Ananconda)\n",
    "# Install the necessary Python libraries (see Python libraries)\n",
    "# Save labeled training and evaluation data in the working directory\n",
    "# Save 'sentence_data.json' from the text categorizer in the working directory\n",
    "# Create the file 'ner_base_config.cfg' at: https://spacy.io/usage/training#quickstart\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# README\n",
    "#------------------------------\n",
    "# The trained model is saved to the folder 'output'\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Python libraries\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create training data\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# This function is based on: https://github.com/amrrs/custom-ner-with-spacy3\n",
    "def create_training_data():\n",
    "    \n",
    "    # get training data\n",
    "    with open('train_data.json') as json_file:\n",
    "        train_data = json.load(json_file)\n",
    "        \n",
    "    # get validation data\n",
    "    with open('valid_data.json') as json_file:\n",
    "        valid_data = json.load(json_file)\n",
    "        \n",
    "    # create training data\n",
    "    print('Create training data:')\n",
    "    nlp = spacy.blank('en')\n",
    "    db = DocBin()\n",
    "    docs = []\n",
    "    for text, annot in tqdm(train_data['annotations']): \n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "                print(text)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents \n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(\"./data/train.spacy\")\n",
    "    \n",
    "    # create validation data\n",
    "    print('Create validation data:')\n",
    "    nlp = spacy.blank('en')\n",
    "    db = DocBin()\n",
    "    docs = []\n",
    "    for text, annot in tqdm(valid_data['annotations']): \n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents \n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(\"./data/valid.spacy\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create config-file\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def create_config():\n",
    "    \n",
    "    !python -m spacy init fill-config ./ner_base_config.cfg ./ner_config.cfg\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Train the NER model\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    !python -m spacy train ner_config.cfg --output ./output\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Span categorizer\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def span_categorizer():\n",
    "    \n",
    "    new_data = {}\n",
    "    new_data['mergers'] = []\n",
    "    \n",
    "    check_manual_data = {}\n",
    "    check_manual_data['mergers'] = []\n",
    "    \n",
    "    \n",
    "    with open('sentence_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    nlp_ner = spacy.load(\"output/model-best\")\n",
    "    nlp = spacy.blank('en')\n",
    "    # Set max document length\n",
    "    nlp.max_length = 3000000\n",
    "    \n",
    "    number_of_mergers = len(data['mergers'])\n",
    "    \n",
    "    pbar = tqdm(total=int(number_of_mergers))\n",
    "    \n",
    "    # calcualte phase of the merger\n",
    "    for merger_index, merger in enumerate(data['mergers']):\n",
    "        new_decisions = []\n",
    "        phase = \"\"\n",
    "        \n",
    "        try:\n",
    "            not_date = datetime.strptime(merger['notification date'], '%d.%m.%Y').date()\n",
    "            dec_date = datetime.strptime(merger['decisions'][0]['decision date'], '%d.%m.%Y').date()\n",
    "            dec_period = (dec_date-not_date).days\n",
    "            \n",
    "            if(dec_period <= 49):\n",
    "                phase = \"Phase I\"\n",
    "            else:\n",
    "                phase = \"Phase II\"\n",
    "        except:\n",
    "            phase = \"Error - Could not calculate phase\"\n",
    "        \n",
    "        # Extract items from the decision texts\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            if(decision):\n",
    "                new_decision_texts = []\n",
    "                                \n",
    "                for decision_text_index, decision_text, in enumerate(decision['decision texts']):\n",
    "                    \n",
    "                    dec_affected_laws = []\n",
    "                    dec_conditions_obligations = []\n",
    "                    dec_violations = []\n",
    "                    dec_decisions = []\n",
    "                    dec_penalties = []\n",
    "                    \n",
    "                    if(decision_text['dec sentences'] == \"\"):\n",
    "                        dec_affected_laws = [\"Error - Please check the pdf-file manually\"]\n",
    "                        dec_conditions_obligations = [\"Error - Please check the pdf-file manually\"]\n",
    "                        dec_violations = [\"Error - Please check the pdf-file manually\"]\n",
    "                        dec_decisions = [\"Error - Please check the pdf-file manually\"]\n",
    "                        dec_penalties = [\"Error - Please check the pdf-file manually\"]\n",
    "\n",
    "                    if(decision_text['dec sentences']):\n",
    "                        text = decision_text['dec sentences']\n",
    "                        \n",
    "                        if(len(text) < nlp.max_length):\n",
    "                            \n",
    "                            doc2 = nlp_ner(text)\n",
    "                            \n",
    "                            for ent in doc2.ents:\n",
    "                                \n",
    "                                if ent.label_ == \"AFFECTED LAW\":\n",
    "                                    dec_affected_laws.append(ent.text)\n",
    "                                elif ent.label_ == \"CONDITIONS & OBLIGATIONS\":\n",
    "                                    dec_conditions_obligations.append(ent.text)\n",
    "                                elif ent.label_ == \"VIOLATION\":\n",
    "                                    dec_violations.append(ent.text)\n",
    "                                elif ent.label_ == \"DECISION\":\n",
    "                                    dec_decisions.append(ent.text)\n",
    "                                    \n",
    "                            fines = re.findall('[Aa](\\stotal)? fine of ((EUR)|(\\\\u20ac)|(ECU)) [0-9]*[0-9\\s,]*(\\smillion)? is\\s?(hereby)? imposed on [\\s\\S]*?(?=( pursuant)|( for))', text)\n",
    "                            periodic_penalty_payments = re.findall('periodic penalty payments .+?(((EUR)|(\\\\u20ac)|(ECU)) [0-9]*[0-9\\s,]*(\\smillion)?)', text)\n",
    "                            \n",
    "                            for fine in fines:\n",
    "                                dec_penalties.append(fine)\n",
    "                                \n",
    "                            for periodic_penalty_payment in periodic_penalty_payments:\n",
    "                                dec_penalties.append(periodic_penalty_payment)\n",
    "                                    \n",
    "                        new_decision_texts.append({\n",
    "                            'language': decision_text['language'],\n",
    "                            'link': decision_text['link'],\n",
    "                            'dec affected laws': dec_affected_laws,\n",
    "                            'dec conditions & obligations': dec_conditions_obligations,\n",
    "                            'dec violations': dec_violations,\n",
    "                            'dec decisions': dec_decisions, \n",
    "                            'dec penalties': dec_penalties\n",
    "                        })\n",
    "                        \n",
    "                new_decisions.append({\n",
    "                    'decision type': decision['decision type'],\n",
    "                    'decision date': decision['decision date'],\n",
    "                    'decision texts': new_decision_texts\n",
    "                })\n",
    "                \n",
    "        new_data['mergers'].append({\n",
    "            'case number': merger['case number'],\n",
    "            'companies': merger['companies'],\n",
    "            'notification date': merger['notification date'],\n",
    "            'NACE': merger['NACE'],\n",
    "            'Phase': phase,\n",
    "            'decisions': new_decisions\n",
    "        })\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "    with open('final_merger_data.json', 'w') as outfile:\n",
    "        json.dump(new_data, outfile, indent = 4)\n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edb5aae-e3ce-4b55-975f-00b2136089b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:04<00:00, 47.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create validation data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 86.23it/s] \n"
     ]
    }
   ],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d894331-592f-4ca0-8d34-cf65e6b8e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "ner_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train ner_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "create_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840add0e-b487-4ae2-95c8-e68d17f35dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-12 15:59:00,602] [INFO] Set up nlp object from config\n",
      "[2022-05-12 15:59:00,617] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-05-12 15:59:00,622] [INFO] Created vocabulary\n",
      "[2022-05-12 15:59:00,628] [INFO] Finished initializing nlp object\n",
      "[2022-05-12 15:59:05,224] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     64.89    0.00    0.00    0.00    0.00\n",
      "  0     200       3429.62  13910.13   57.97   61.54   54.79    0.58\n",
      "  2     400       2086.34   1107.08   71.79   67.47   76.71    0.72\n",
      "  3     600       3038.84    803.24   79.08   75.62   82.88    0.79\n",
      "  4     800       2373.15    608.47   83.92   85.71   82.19    0.84\n",
      "  5    1000      10292.31    755.44   83.28   79.87   86.99    0.83\n",
      "  7    1200      26459.18   1167.06   86.62   89.13   84.25    0.87\n",
      "  9    1400      15090.89    496.00   91.10   91.10   91.10    0.91\n",
      " 11    1600       8886.76    382.73   90.97   92.25   89.73    0.91\n",
      " 14    1800        302.42    164.71   89.29   93.28   85.62    0.89\n",
      " 17    2000        999.40    196.72   89.51   91.43   87.67    0.90\n",
      " 20    2200        347.23    152.56   89.42   89.12   89.73    0.89\n",
      " 24    2400        686.68    176.17   92.63   94.96   90.41    0.93\n",
      " 29    2600        696.03    169.36   92.31   94.29   90.41    0.92\n",
      " 34    2800        404.45    138.15   87.32   89.86   84.93    0.87\n",
      " 39    3000        475.67    123.87   89.75   92.70   86.99    0.90\n",
      " 44    3200        817.88    131.95   89.51   91.43   87.67    0.90\n",
      " 50    3400       1909.48    125.15   89.90   91.49   88.36    0.90\n",
      " 55    3600        446.44    107.66   88.19   89.44   86.99    0.88\n",
      " 60    3800       1422.60    116.85   90.97   92.25   89.73    0.91\n",
      " 66    4000        743.33    130.09   85.91   86.21   85.62    0.86\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ada8a11-4a16-481b-817e-d5928335a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8447/8447 [02:40<00:00, 52.51it/s] \n"
     ]
    }
   ],
   "source": [
    "span_categorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed110911-784e-4573-8166-6ad76ea2deb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
