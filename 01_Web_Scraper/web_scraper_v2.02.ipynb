{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6225e3f-71a4-437d-8fe7-f98f686f1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Script for scraping merger cases on the European Commission competition website\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Author: Patrik Schwalm\n",
    "# E-Mail: schwapa3@students.zhaw.ch\n",
    "# Last update: 29.04.2022\n",
    "# Version 2.02\n",
    "\n",
    "#------------------------------\n",
    "# Setup\n",
    "#------------------------------\n",
    "# Install Python (Ananconda)\n",
    "# Install the necessary Python libraries (see Python libraries)\n",
    "# Install Google Chrome\n",
    "# Download ChromeDriver (https://chromedriver.chromium.org/downloads)\n",
    "# Save the ChromeDriver in the working directory\n",
    "# Create a file called 'user_agents.txt' with user agents\n",
    "# Create a folder called 'tmp' in the working directory\n",
    "\n",
    "#------------------------------\n",
    "# README\n",
    "#------------------------------\n",
    "# The web scraper saves the data in a JSON-file with the following naming convention '%d%m%Y-%H%M%S.json'\n",
    "# The web scraper will write the data to the file once all the data is scraped\n",
    "# This means the web scraper cannot be paused\n",
    "# It takes around 9 hours for the web scarper to finish\n",
    "# The created JSON-file needs roughly 500 MB of disk space\n",
    "# On the European Commission competition website are 6 PDF-files that are not searchable (as of 06.02.2022)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Python libraries\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# User agents\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "ua_path = 'user_agents.txt'\n",
    "ua_list = [line.rstrip('\\n') for line in open(ua_path)]\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "# Driver settings\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "def new_chrome_driver(headless = False):\n",
    "    opts = Options()\n",
    "    opts.add_argument('user-agent=' + random.choice(ua_list))\n",
    "    \n",
    "    # Driver headless (True/False)\n",
    "    if headless == True:\n",
    "        opts.add_argument('--headless')\n",
    "    else:\n",
    "        opts.add_argument('--window-size=1200,800')\n",
    "    \n",
    "    # Driver\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver.exe', options=opts)\n",
    "    \n",
    "    return (driver)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Download PDF-file and extract the text\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def pdf_handler(link, language):\n",
    "    \n",
    "    # Check if language is empty\n",
    "    if language == '':\n",
    "        language = 'N/A' \n",
    "    \n",
    "    # Filter out corrupt links without PDF-files\n",
    "    if link == 'https://ec.europa.eu/competition/mergers//':                      \n",
    "\n",
    "        text = 'Error - link does not point to a valid PDF-file'\n",
    "                            \n",
    "    else:                       \n",
    "    \n",
    "        headers = {\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "                    \"Accept-Language\": \"de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "                    \"Cache-Control\": \"max-age=0\",\n",
    "                    \"Connection\": \"keep-alive\",\n",
    "                    \"Host\": \"ec.europa.eu\",\n",
    "                    \"Referer\": \"https://ec.europa.eu/competition/elojade/isef/index.cfm?fuseaction=dsp_result&policy_area_id=2\",\n",
    "                    \"sec-ch-ua\": '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"97\", \"Chromium\";v=\"97\"',\n",
    "                    \"sec-ch-ua-mobile\": \"?0\",\n",
    "                    \"sec-ch-ua-platform\": '\"Windows\"',\n",
    "                    \"Sec-Fetch-Dest\": \"document\",\n",
    "                    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "                    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "                    \"Sec-Fetch-User\": \"?1\",\n",
    "                    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "                    \"User-Agent\": random.choice(ua_list)\n",
    "                }\n",
    "\n",
    "        delay = 5\n",
    "        max_retries = 10\n",
    "\n",
    "        # Download the PDF-file\n",
    "        # Retry to download the PDF-file after an ConnectionResetError 10054 with increased delay\n",
    "        # ConnectionResetError 10054 may occur at irregular intervals\n",
    "        # The error handling of the ConnectionResetError 10054 is based on: https://stackoverflow.com/a/67499291\n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(link, headers=headers)\n",
    "                file_path = os.path.join('./tmp', os.path.basename(link))\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)                  \n",
    "                break\n",
    "            except:\n",
    "                time.sleep(delay)\n",
    "                delay *=2\n",
    "                print('Information: ConnectionResetError 10054 occured')\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "        try:\n",
    "            text = ''\n",
    "            # Extract text from the PDF-file\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "            \n",
    "            # Handle PDF-files that are not searchable\n",
    "            if text == '':\n",
    "                text = 'Error - PDF-file is not searchable'\n",
    "            \n",
    "            page.close()\n",
    "            # Delete the PDF-file(s) to save space\n",
    "            os.remove(file_path)\n",
    "\n",
    "        # Links to the PDF-files can be corrupt and do not point to a PDF-file\n",
    "        # If that is the case the generated PDF-file can not be opened with pdfplumber and an eror is thrown\n",
    "        except:\n",
    "\n",
    "            text = 'Error - link does not point to a valid PDF-file'\n",
    "                \n",
    "    return(language, text)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Scraping functionalities\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def eu_merger_scraper(case_number_filter = 'none'):\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Starting the script ...Please wait')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    # Initialisation\n",
    "    driver = new_chrome_driver()\n",
    "    date_time = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "    last_site = False\n",
    "    data = {}\n",
    "    data['mergers'] = []\n",
    "    \n",
    "    # Access the website and delete all cookies\n",
    "    driver.get('https://ec.europa.eu/competition/elojade/isef/index.cfm?clear=1&policy_area_id=2')\n",
    "    driver.delete_all_cookies()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # 'Select All' Decision Types\n",
    "    driver.find_element_by_xpath('//*[@id=\"adv_search_2\"]/fieldset/div[3]/a[1]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # Deselct the 'Decision Types' without an 'Article'\n",
    "    no_art_decisions = driver.find_elements_by_xpath('//*[@id=\"decision_type_id_2\"]/option[not(contains(text(),\"Art.\"))]')\n",
    "    \n",
    "    for no_art_decision in no_art_decisions:\n",
    "        ActionChains(driver) \\\n",
    "            .key_down(Keys.CONTROL) \\\n",
    "            .click(no_art_decision) \\\n",
    "            .key_up(Keys.CONTROL) \\\n",
    "            .perform()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Define filter 'Case Number' --> this is used for testing purposes\n",
    "    if case_number_filter != 'none':\n",
    "        driver.find_element_by_xpath('//*[@id=\"case_number\"]').send_keys(case_number_filter)\n",
    "    \n",
    "    # Click 'Search' Button\n",
    "    driver.find_element_by_xpath('//*[@id=\"BodyContent\"]/form/input[1]').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get the number of mergers\n",
    "    number_of_mergers = driver.find_element_by_xpath('//*[@id=\"BodyContent\"]/table[3]/tbody/tr[2]/td').text[10:]\n",
    "    pbar = tqdm(total=int(number_of_mergers))\n",
    "    \n",
    "    #------------------------------\n",
    "    # Loop through all the pages\n",
    "    #------------------------------\n",
    "    while not last_site:\n",
    "        \n",
    "        # Open all the 'Show details' tabs\n",
    "        details = driver.find_elements_by_class_name('details')\n",
    "        while details:\n",
    "            \n",
    "            for detail in details:\n",
    "                detail.click()\n",
    "                time.sleep(0.4)\n",
    "            \n",
    "            # Check if all 'Show details' tabs are opened\n",
    "            time.sleep (1)\n",
    "            details = driver.find_elements_by_xpath('//input[@value=\"Show details\"]')\n",
    "        \n",
    "        # Get all the mergers on the page\n",
    "        mergers =  driver.find_elements_by_class_name('test')\n",
    "        \n",
    "        # Loop through all the mergers on the current page\n",
    "        for merger in mergers:\n",
    "            \n",
    "            # Get the 'Case Number' of the merger\n",
    "            case_number = merger.find_element_by_xpath('.//p/span/strong/a[1]').text\n",
    "            \n",
    "            companies = []\n",
    "            # Get all the companies names\n",
    "            sel_companies = merger.find_elements_by_xpath('.//p/span/strong/a[position()>=2]')\n",
    "            # Append each company to the list 'companies'\n",
    "            for sel_company in sel_companies:\n",
    "                companies.append(sel_company.text)\n",
    "            \n",
    "            # Check if the merger has a notification date and if so get the notification date\n",
    "            if merger.find_elements_by_xpath('.//*[@class=\"details\"]/tbody/tr[./td[contains(text(),\"Notification on\")]]/td[2]'):\n",
    "                notification_date = merger.find_element_by_xpath('.//*[@class=\"details\"]/tbody/tr[./td[contains(text(),\"Notification on\")]]/td[2]').text\n",
    "            else:\n",
    "                notification_date = 'N/A'\n",
    "                \n",
    "            nace_codes = []\n",
    "            # Get all the 'NACE' codes\n",
    "            sel_nace_codes = merger.find_elements_by_xpath('.//*[@class=\"details\"]/tbody/tr[./td[contains(text(),\"NACE\")]]/td[2]/a')\n",
    "            # Append each 'NACE' code to the list 'nace_codes'\n",
    "            for sel_nace_code in sel_nace_codes:\n",
    "                nace_codes.append(sel_nace_code.text)\n",
    "                \n",
    "            decisions =[]\n",
    "            # Get all 'Decisions'\n",
    "            sel_decisions = merger.find_elements_by_xpath('.//*[@id=\"decisions\"]/tbody/tr[./td[2]/strong]')\n",
    "            \n",
    "            # Loop through all 'Decisions' \n",
    "            for sel_decision_index, sel_decision in enumerate(sel_decisions):\n",
    "                \n",
    "                # Only select 'Decisions' containing the text 'Art'\n",
    "                if sel_decision.find_elements_by_xpath('.//td[2]/strong[contains(text(),\"Art\")]'):\n",
    "                    \n",
    "                    # Get the decision type\n",
    "                    decision_type = sel_decision.find_element_by_xpath('.//td[2]/strong[contains(text(),\"Art\")]').text\n",
    "                    # Get the decision date\n",
    "                    decision_date = sel_decision.find_element_by_xpath('.//td[1]/strong').text\n",
    "                    \n",
    "                    decision_texts = []\n",
    "                    # Get all the decision texts\n",
    "                    sel_decision_texts = sel_decision.find_elements_by_xpath('.//following-sibling::tr[./td[3]/table/tbody/tr[*]/td[2]/a and count(preceding-sibling::tr[./td[2]/strong])='+str(sel_decision_index+1)+'][1]/td[3]/table/tbody/tr[*]/td[2]/a')\n",
    "                    \n",
    "                    # Check if decision text where found\n",
    "                    if sel_decision_texts:\n",
    "\n",
    "                        # Loop through all decsion texts\n",
    "                        for sel_decision_text in sel_decision_texts:\n",
    "\n",
    "                            link = sel_decision_text.get_attribute(\"href\")\n",
    "                            language = sel_decision_text.text\n",
    "\n",
    "                            language, text = pdf_handler(link, language)\n",
    "\n",
    "                            # Append the decision text information to the list 'decision_texts'\n",
    "                            decision_texts.append({\n",
    "                                'language': language,\n",
    "                                'link': link,\n",
    "                                'text': text\n",
    "                            })\n",
    "                            \n",
    "                    # Append the decision information to the list 'decisions'\n",
    "                    decisions.append({\n",
    "                        'decision type': decision_type,\n",
    "                        'decision date': decision_date,\n",
    "                        'decision texts': decision_texts\n",
    "                    })\n",
    "                \n",
    "            # Append the merger information to the list 'mergers'\n",
    "            data['mergers'].append({\n",
    "                'case number': case_number,\n",
    "                'companies': companies,\n",
    "                'notification date': notification_date,\n",
    "                'NACE': nace_codes,\n",
    "                'decisions': decisions\n",
    "            })\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Check if the last site was reached\n",
    "        if driver.find_elements_by_xpath('//*[@id=\"BodyContent\"]/table[3]/tbody/tr[1]/td[@style=\"display:none\"]/input[@value=\"Next\"]'):\n",
    "            last_site = True\n",
    "        else:\n",
    "            driver.find_element_by_xpath('//*[@id=\"BodyContent\"]/table[3]/tbody/tr[1]/td[3]/input').click()\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Writing data to JSON-File ...Please wait')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    # Write all the scraped data in the JSON-file\n",
    "    with open('data' + date_time + '.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent = 4)\n",
    "        \n",
    "    print('----------------------------------------------------')\n",
    "    print('JSON-File successfully saved')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    # delete corrupted PDF-files from the 'tmp'-folder\n",
    "    folder_path = './tmp'\n",
    "    for f in os.listdir(folder_path):\n",
    "        if f.endswith('.pdf'):\n",
    "            os.remove(os.path.join(folder_path, f))\n",
    "    \n",
    "    # Close driver\n",
    "    driver.close()\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Script sucessfully terminated')\n",
    "    print('----------------------------------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6f4d8-b40f-4628-ba93-45ad15c31261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Starting the script ...Please wait\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 237/8489 [26:40<9:03:08,  3.95s/it]  "
     ]
    }
   ],
   "source": [
    "eu_merger_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
