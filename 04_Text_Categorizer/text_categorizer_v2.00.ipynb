{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea802ab4-f251-48e5-8c83-76c788d41ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Extract text with the \"relevant\" information from decision texts\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Author: Patrik Schwalm\n",
    "# E-Mail: schwapa3@students.zhaw.ch\n",
    "# Last update: 10.05.2022\n",
    "# Version 2.00\n",
    "\n",
    "#------------------------------\n",
    "# Setup\n",
    "#------------------------------\n",
    "# Install Python (Ananconda)\n",
    "# Install the necessary Python libraries (see Python libraries)\n",
    "# Save the 'final_train_data.json' file and the 'final_valid_data.json' in the working directory\n",
    "# Save the file 'clean_input_data.json' from the data cleaner in the working directory\n",
    "# Create the file 'textcat_base_config.cfg' at: https://spacy.io/usage/training#quickstart\n",
    "\n",
    "#------------------------------\n",
    "# README\n",
    "#------------------------------\n",
    "# The trained model is saved to the folder 'output'\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Python libraries\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Set categories for labeled data\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# This function is based on: https://towardsdatascience.com/building-sentiment-classifier-using-spacy-3-0-transformers-c744bfc767b\n",
    "def make_docs(data):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        if label == 'beg':\n",
    "            doc.cats['beg'] = 1\n",
    "            doc.cats['end'] = 0\n",
    "            doc.cats['out'] = 0\n",
    "        elif label == 'end':\n",
    "            doc.cats['beg'] = 0\n",
    "            doc.cats['end'] = 1\n",
    "            doc.cats['out'] = 0\n",
    "        else:\n",
    "            doc.cats['beg'] = 0\n",
    "            doc.cats['end'] = 0\n",
    "            doc.cats['out'] = 1\n",
    "            \n",
    "        docs.append(doc)\n",
    "    \n",
    "    return(docs)       \n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create training and validation data\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# This function is based on: https://towardsdatascience.com/building-sentiment-classifier-using-spacy-3-0-transformers-c744bfc767b\n",
    "# This function is based on : https://medium.com/analytics-vidhya/building-a-text-classifier-with-spacy-3-0-dd16e9979a\n",
    "def create_training_data():\n",
    "    \n",
    "    with open('final_train_data.json') as json_file:\n",
    "        train_data = json.load(json_file)\n",
    "    \n",
    "    with open('final_valid_data.json') as json_file:\n",
    "        valid_data = json.load(json_file)\n",
    "        \n",
    "    print('Create training data:')\n",
    "    train_docs = make_docs(train_data)\n",
    "    doc_bin = DocBin(docs=train_docs)\n",
    "    doc_bin.to_disk(\"./data/train.spacy\")\n",
    "    \n",
    "    print('Create validation data:')\n",
    "    valid_docs = make_docs(valid_data)\n",
    "    doc_bin = DocBin(docs=valid_docs)\n",
    "    doc_bin.to_disk(\"./data/valid.spacy\")\n",
    "    \n",
    "#-------------------------------------------------------------------------------------\n",
    "# Create the config file\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def create_config():\n",
    "    \n",
    "    !python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Train the model\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    !python -m spacy train textcat_config.cfg --output ./output\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Text catgorizer\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def text_categorizer():\n",
    "    \n",
    "    new_data = {}\n",
    "    new_data['mergers'] = []\n",
    "    \n",
    "    with open('clean_input_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    nlp_textcat = spacy.load(\"output/model-best\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    nlp.max_length = 3000000\n",
    "    skipped_files_counter = 0\n",
    "    \n",
    "    number_of_mergers = len(data['mergers'])\n",
    "    \n",
    "    pbar = tqdm(total=int(number_of_mergers))\n",
    "    \n",
    "    for merger_index, merger in enumerate(data['mergers']):\n",
    "        new_decisions = []\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            if(decision):\n",
    "                new_decision_texts = []\n",
    "                \n",
    "                for decision_text_index, decision_text, in enumerate(decision['decision texts']):\n",
    "                    \n",
    "                    dec_sentences = []\n",
    "                    \n",
    "                    if(decision_text['text']):\n",
    "                        text = decision_text['text']\n",
    "                        \n",
    "                        if(len(text) < nlp.max_length):\n",
    "                            \n",
    "                            doc = nlp(text)\n",
    "                            is_important = False\n",
    "\n",
    "                            for sent in doc.sents:\n",
    "                                sentence = sent.text\n",
    "                                doc2 = nlp_textcat(sentence)\n",
    "                                cats = doc2.cats\n",
    "                                beg = cats['beg']\n",
    "                                end = cats['end']\n",
    "                                out = cats['out']\n",
    "                                \n",
    "                                if (beg > end) and (beg > out):\n",
    "                                    if not(is_important):\n",
    "                                        is_important = True\n",
    "                                    else:\n",
    "                                         dec_sentences.clear()\n",
    "                                    if (dec_sentences):\n",
    "                                        dec_sentences.clear()\n",
    "                                    dec_sentences.append(sentence)\n",
    "                                elif (end > beg) and (end > out):\n",
    "                                    if(is_important):\n",
    "                                        dec_sentences.append(sentence)\n",
    "                                    is_important = False\n",
    "                                else:\n",
    "                                    if is_important == True:\n",
    "                                        dec_sentences.append(sentence)\n",
    "                        else:\n",
    "                            print('Error - Text was too big to be analyzed')\n",
    "                        \n",
    "                        final_dec_sentences = \"\"\n",
    "                        for sentence in dec_sentences:\n",
    "                            final_dec_sentences = final_dec_sentences + \" \" + sentence                           \n",
    "                        \n",
    "                        new_decision_texts.append({\n",
    "                            'language': decision_text['language'],\n",
    "                            'link': decision_text['link'],\n",
    "                            'dec sentences': final_dec_sentences\n",
    "                        })\n",
    "                    \n",
    "                new_decisions.append({\n",
    "                    'decision type': decision['decision type'],\n",
    "                    'decision date': decision['decision date'],\n",
    "                    'decision texts': new_decision_texts\n",
    "                })\n",
    "                    \n",
    "        new_data['mergers'].append({\n",
    "            'case number': merger['case number'],\n",
    "            'companies': merger['companies'],\n",
    "            'notification date': merger['notification date'],\n",
    "            'NACE': merger['NACE'],\n",
    "            'decisions': new_decisions\n",
    "        })\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "    with open('sentence_data.json', 'w') as outfile:\n",
    "        json.dump(new_data, outfile, indent = 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad64c1a4-c109-4c6f-af98-dc8eb5ff7e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [00:12<00:00, 116.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create validation data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:03<00:00, 115.09it/s]\n"
     ]
    }
   ],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654dae77-534f-4c1e-8dee-e78125906ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "create_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ce0cd4-2dde-44ee-a861-b719dc6ae96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['textcat']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.03        0.00    0.00\n",
      "  0     200         12.59       93.02    0.93\n",
      "  0     400          7.25       94.78    0.95\n",
      "  1     600          1.16       96.40    0.96\n",
      "  2     800          0.46       97.14    0.97\n",
      "  3    1000          0.20       97.17    0.97\n",
      "  4    1200          0.09       97.32    0.97\n",
      "  6    1400          0.05       97.91    0.98\n",
      "  8    1600          0.02       97.91    0.98\n",
      " 10    1800          0.01       97.91    0.98\n",
      " 13    2000          0.01       98.18    0.98\n",
      " 16    2200          0.01       98.05    0.98\n",
      " 21    2400          0.00       98.33    0.98\n",
      " 25    2600          0.00       98.47    0.98\n",
      " 29    2800          0.00       98.47    0.98\n",
      " 34    3000          0.00       98.47    0.98\n",
      " 38    3200          0.00       98.61    0.99\n",
      " 43    3400          0.00       98.61    0.99\n",
      " 47    3600          0.00       98.61    0.99\n",
      " 52    3800          0.00       98.61    0.99\n",
      " 56    4000          0.00       98.61    0.99\n",
      " 61    4200          0.00       98.61    0.99\n",
      " 65    4400          0.00       98.75    0.99\n",
      " 69    4600          0.00       98.61    0.99\n",
      " 74    4800          0.00       98.61    0.99\n",
      " 78    5000          0.00       98.61    0.99\n",
      " 83    5200          0.00       98.89    0.99\n",
      " 87    5400          0.00       98.75    0.99\n",
      " 92    5600          0.00       98.89    0.99\n",
      " 96    5800          0.00       98.89    0.99\n",
      "100    6000          0.00       98.89    0.99\n",
      "105    6200          0.00       99.03    0.99\n",
      "109    6400          0.00       98.89    0.99\n",
      "114    6600          0.00       98.89    0.99\n",
      "118    6800          0.00       98.47    0.98\n",
      "123    7000          0.00       98.61    0.99\n",
      "127    7200          0.00       98.75    0.99\n",
      "131    7400          0.00       98.61    0.99\n",
      "136    7600          0.00       98.61    0.99\n",
      "140    7800          0.00       98.61    0.99\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-07 15:04:14,679] [INFO] Set up nlp object from config\n",
      "[2022-05-07 15:04:14,705] [INFO] Pipeline: ['textcat']\n",
      "[2022-05-07 15:04:14,714] [INFO] Created vocabulary\n",
      "[2022-05-07 15:04:14,731] [INFO] Finished initializing nlp object\n",
      "[2022-05-07 15:04:18,703] [INFO] Initialized pipeline components: ['textcat']\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406c9850-1f2e-4623-bdbe-04f913c67ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8447/8447 [7:50:04<00:00,  3.34s/it]     \n"
     ]
    }
   ],
   "source": [
    "text_categorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba66d4-32b4-43df-9553-ea5485267b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3e5f7-9b84-4718-8979-4d6238a4ac51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
