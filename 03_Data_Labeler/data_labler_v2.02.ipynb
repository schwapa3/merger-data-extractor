{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbb477e-1d96-4dc8-b543-7f39bb56e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Create labeled data from the file clean_train_data.json from the data_cleaner_vx.xx\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Author: Patrik Schwalm\n",
    "# E-Mail: schwapa3@students.zhaw.ch\n",
    "# Last update: 01.05.2022\n",
    "# Version 2.02\n",
    "\n",
    "#------------------------------\n",
    "# Setup\n",
    "#------------------------------\n",
    "# Install Python (Ananconda)\n",
    "# Install the necessary Python libraries (see Python libraries)\n",
    "# Save the JSON-file 'clean_train_data.json' from the data cleaner in the in the working directory\n",
    "\n",
    "#------------------------------\n",
    "# README\n",
    "#------------------------------\n",
    "# The labeled data has still to be manually verified by a human\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Python libraries\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Read data from JSON-file\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# get clean_train_data.json\n",
    "def get_json_file(filename = 'clean_train_data.json'):\n",
    "    # read JSON-file\n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Labele the sentences of the deicion texts\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def label_data(data):\n",
    "    \n",
    "    # Initialisation\n",
    "    beg_labeled_data = {}\n",
    "    end_labeled_data = {}\n",
    "    out_labeled_data = {}\n",
    "    beg_labeled_data['labeled datasets'] = []\n",
    "    end_labeled_data['labeled datasets'] = []\n",
    "    out_labeled_data['labeled datasets'] = []\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    # Set max document length\n",
    "    nlp.max_length = 3000000\n",
    "    skipped_files_counter = 0\n",
    "    \n",
    "    for merger_index, merger in tqdm(enumerate(data['mergers']), total = len(data['mergers'])):\n",
    "        case_number = merger['case number']\n",
    "        for decision_index, decision in enumerate(merger['decisions']):\n",
    "            decision_type = decision['decision type']\n",
    "            for decision_text_index, decision_text in enumerate(decision['decision texts']):\n",
    "                decision_link = decision_text['link']\n",
    "                decision_text = decision_text['text']\n",
    "                \n",
    "                if(len(decision_text) < nlp.max_length):\n",
    "                   \n",
    "                    doc = nlp(decision_text)\n",
    "\n",
    "                    for sent in doc.sents:\n",
    "                        if(\n",
    "                            re.search('[Hh][Aa][Ss]\\s[Aa]-?[Dd][Oo][Pp][Tt]-?[Ee][Dd]\\s[Tt][Hh][Ii][Ss]\\s[Dd][Ee]-?[Cc][Ii]-?[Ss][Ii][Oo][Nn]', sent.text) \\\n",
    "                            or re.search('For (the|these) above (mentioned)?\\s?reasons,? the (European)?\\s?Commission has (therefore)?\\s?(decided|(come to the conclusion))', sent.text) \\\n",
    "                            or re.search('The Commission has therefore decided', sent.text) \\\n",
    "                            or re.search('For the reasons set out in the Notice on a simplified procedure,? the European Commission has decided', sent.text) \\\n",
    "                            or re.search('In (light|view) of the above,? the Commission', sent.text) \\\n",
    "                            or re.search('For the above reasons,? subject to full compliance with the commitments submitted by the notifying party, the Commission has decided', sent.text) \\\n",
    "                            or re.search('Based on the above considerations,? the Commission has decided', sent.text) \\\n",
    "                            or re.search('For these reasons,? the Commission concludes', sent.text) \\\n",
    "                            or re.search('For the above reasons,? and given that [\\s\\S]* expressed its agreement,? ', sent.text)\n",
    "                        ):\n",
    "                            beg_labeled_data['labeled datasets'].append({\n",
    "                                'text': sent.text,\n",
    "                                'label': 'beg',\n",
    "                                'decision type': decision_type,\n",
    "                                'link': decision_link,\n",
    "                                'case number': case_number\n",
    "                            })\n",
    "                        elif(\n",
    "                            re.search('For the Commission', sent.text) \\\n",
    "                            or re.search('ELECTRONICALLY RE-CREATED TEXT', sent.text)\n",
    "                        ):\n",
    "                            end_labeled_data['labeled datasets'].append({\n",
    "                                'text': sent.text,\n",
    "                                'label': 'end',\n",
    "                                'decision type': decision_type,\n",
    "                                'link': decision_link,\n",
    "                                'case number': case_number\n",
    "                            })\n",
    "                        else:\n",
    "                            out_labeled_data['labeled datasets'].append({\n",
    "                                'text': sent.text,\n",
    "                                'label': 'out',\n",
    "                                'decision type': decision_type,\n",
    "                                'link': decision_link,\n",
    "                                'case number': case_number\n",
    "                            })\n",
    "                else:\n",
    "                    skipped_files_counter += 1\n",
    "                                    \n",
    "    return(beg_labeled_data, end_labeled_data, out_labeled_data, skipped_files_counter)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# Data labeler\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def en_data_labler():\n",
    "    data = get_json_file()\n",
    "    beg_labeled_data, end_labeled_data, out_labeled_data, skipped_files_counter = label_data(data)\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Writing data to JSON-File ...Please wait')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    with open('beg.json', 'w') as outfile:\n",
    "        json.dump(beg_labeled_data, outfile, indent = 4)\n",
    "    \n",
    "    with open('end.json', 'w') as outfile:\n",
    "        json.dump(end_labeled_data, outfile, indent = 4)\n",
    "\n",
    "    with open('out.json', 'w') as outfile:\n",
    "        json.dump(out_labeled_data, outfile, indent = 4)\n",
    "        \n",
    "    print('----------------------------------------------------')\n",
    "    print('JSON-File successfully saved')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Skipped '+str(skipped_files_counter)+' files because they were too large.')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Script sucessfully terminated')\n",
    "    print('----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e5b79-fc73-4281-9517-fb0c318ba36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6946/6946 [7:52:10<00:00,  4.08s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Writing data to JSON-File ...Please wait\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "en_data_labler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
